{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69a9f504-e44a-4547-9b8a-04a5ab30b5df",
   "metadata": {},
   "source": [
    "Q1>>H: What is a Support Vector Machine (SVM)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d0beef-8916-4028-985a-e18dcf62f1cc",
   "metadata": {},
   "source": [
    "A Support Vector Machine (SVM) is a supervised machine learning algorithm primarily used for classification tasks, although it can also be adapted for regression. The main idea behind SVM is to find a hyperplane that best separates data points of different classes in a high-dimensional space.\n",
    "\n",
    "Here are some key concepts related to SVM:\n",
    "\n",
    "Hyperplane: In an n-dimensional space, a hyperplane is a flat affine subspace of dimension n-1. In the context of SVM, the hyperplane is the decision boundary that separates different classes.\n",
    "\n",
    "Support Vectors: These are the data points that are closest to the hyperplane. They are critical in defining the position and orientation of the hyperplane. The SVM algorithm focuses on these points because they are the most informative for the classification task.\n",
    "\n",
    "Margin: The margin is the distance between the hyperplane and the nearest data points from either class (the support vectors). SVM aims to maximize this margin, which helps improve the model's generalization to unseen data.\n",
    "\n",
    "Kernel Trick: SVM can efficiently perform non-linear classification using a technique called the kernel trick. This involves transforming the input data into a higher-dimensional space where a linear hyperplane can be used to separate the classes. Common kernel functions include linear, polynomial, and radial basis function (RBF) kernels.\n",
    "\n",
    "Soft Margin: In real-world scenarios, data may not be perfectly separable. SVM can incorporate a soft margin, allowing some misclassifications to achieve better overall performance. This is controlled by a regularization parameter that balances the trade-off between maximizing the margin and minimizing classification errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d6f3e8-d028-45ab-81bd-11315e703d35",
   "metadata": {},
   "source": [
    "Q2>>What is the difference between Hard Margin and Soft Margin SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff511c3c-5661-4e8a-a694-c1592effb4fc",
   "metadata": {},
   "source": [
    "The difference between Hard Margin and Soft Margin Support Vector Machines (SVM) primarily lies in how they handle data that is not perfectly separable. Here’s a breakdown of the two concepts:\n",
    "\n",
    "Hard Margin SVM\n",
    "Definition: Hard Margin SVM is used when the data is linearly separable, meaning that there exists a hyperplane that can perfectly separate the classes without any misclassifications.\n",
    "\n",
    "Constraints: In Hard Margin SVM, the algorithm seeks to find a hyperplane that maximizes the margin while ensuring that all data points are correctly classified. This means that no data points can be on the wrong side of the hyperplane.\n",
    "\n",
    "Limitations:\n",
    "\n",
    "Hard Margin SVM is sensitive to outliers and noise. If there are any misclassified points or outliers, it may not be able to find a suitable hyperplane.\n",
    "It is not suitable for datasets that are not perfectly separable, as it would require an infinite margin, which is not feasible.\n",
    "Use Case: Hard Margin SVM is typically used in scenarios where the data is clean and well-separated, such as in some controlled environments or synthetic datasets.\n",
    "\n",
    "Soft Margin SVM\n",
    "Definition: Soft Margin SVM allows for some misclassifications in order to achieve a better overall model. It introduces a penalty for misclassified points, enabling the algorithm to find a balance between maximizing the margin and minimizing classification errors.\n",
    "\n",
    "Constraints: In Soft Margin SVM, the algorithm allows some data points to be on the wrong side of the hyperplane. The degree of misclassification is controlled by a regularization parameter (often denoted as (C)):\n",
    "\n",
    "A small (C) value allows for a larger margin but permits more misclassifications.\n",
    "A large (C) value emphasizes correct classification, leading to a smaller margin.\n",
    "Advantages:\n",
    "\n",
    "Soft Margin SVM is more robust to outliers and noise, making it suitable for real-world datasets that may not be perfectly separable.\n",
    "It provides greater flexibility in finding a decision boundary that generalizes better to unseen data.\n",
    "Use Case: Soft Margin SVM is widely used in practical applications where data is noisy or not perfectly separable, such as in text classification, image recognition, and other complex datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3abdc1-bebc-4484-8021-15c3d3338687",
   "metadata": {},
   "source": [
    "Q3>>What is the mathematical intuition behind SVM4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27152c63-fce4-4377-9daf-576cea895db3",
   "metadata": {},
   "source": [
    "The mathematical intuition behind Support Vector Machines (SVM) revolves around the concepts of hyperplanes, margins, and optimization. Here’s a breakdown of the key mathematical components that underpin SVM:\n",
    "\n",
    "1. Hyperplane Definition\n",
    "In an (n)-dimensional space, a hyperplane can be defined by the equation:\n",
    "\n",
    "[ \\mathbf{w} \\cdot \\mathbf{x} + b = 0 ]\n",
    "\n",
    "where:\n",
    "\n",
    "(\\mathbf{w}) is the weight vector (normal to the hyperplane),\n",
    "(\\mathbf{x}) is the input feature vector,\n",
    "(b) is the bias term.\n",
    "The hyperplane divides the space into two halves, each corresponding to a different class.\n",
    "\n",
    "2. Classification\n",
    "For a given input (\\mathbf{x}), the classification decision can be made using the sign of the function:\n",
    "\n",
    "[ f(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b ]\n",
    "\n",
    "If (f(\\mathbf{x}) > 0), classify (\\mathbf{x}) as one class (e.g., +1).\n",
    "If (f(\\mathbf{x}) < 0), classify (\\mathbf{x}) as the other class (e.g., -1).\n",
    "3. Margin Maximization\n",
    "The margin is defined as the distance between the hyperplane and the nearest data points from either class (the support vectors). The goal of SVM is to maximize this margin.\n",
    "\n",
    "Margin Calculation\n",
    "The margin (M) can be expressed as:\n",
    "\n",
    "[ M = \\frac{2}{|\\mathbf{w}|} ]\n",
    "\n",
    "To maximize the margin, we need to minimize (|\\mathbf{w}|) while ensuring that the data points are correctly classified. This leads to the following constraints for the support vectors:\n",
    "\n",
    "For points in class +1: (\\mathbf{w} \\cdot \\mathbf{x}_i + b \\geq 1)\n",
    "For points in class -1: (\\mathbf{w} \\cdot \\mathbf{x}_i + b \\leq -1)\n",
    "These constraints can be combined into a single constraint:\n",
    "\n",
    "[ y_i (\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1 \\quad \\forall i ]\n",
    "\n",
    "where (y_i) is the label of the data point ((+1) or (-1)).\n",
    "\n",
    "4. Optimization Problem\n",
    "The SVM optimization problem can be formulated as:\n",
    "\n",
    "[ \\text{Minimize} \\quad \\frac{1}{2} |\\mathbf{w}|^2 ]\n",
    "\n",
    "subject to the constraints:\n",
    "\n",
    "[ y_i (\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1 \\quad \\forall i ]\n",
    "\n",
    "This is a convex optimization problem, and it can be solved using techniques such as Lagrange multipliers.\n",
    "\n",
    "5. Soft Margin SVM\n",
    "In cases where the data is not perfectly separable, we introduce slack variables (\\xi_i) to allow for some misclassifications:\n",
    "\n",
    "[ y_i (\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1 - \\xi_i ]\n",
    "\n",
    "The optimization problem then becomes:\n",
    "\n",
    "[ \\text{Minimize} \\quad \\frac{1}{2} |\\mathbf{w}|^2 + C \\sum_{i=1}^{N} \\xi_i ]\n",
    "\n",
    "where (C) is a regularization parameter that controls the trade-off between maximizing the margin and minimizing the classification error.\n",
    "\n",
    "6. Kernel Trick\n",
    "For non-linearly separable data, SVM can use the kernel trick to transform the input space into a higher-dimensional space where a linear hyperplane can be used to separate the classes. The kernel function (K(\\mathbf{x}_i, \\mathbf{x}_j)) computes the inner product in this transformed space without explicitly mapping the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b56d419-bcf5-4b64-a950-66156bdfcccf",
   "metadata": {},
   "source": [
    "Q4>>What is the role of Lagrange Multipliers in SVM4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a216dae6-88ea-4708-b444-3aab66409044",
   "metadata": {},
   "source": [
    "Lagrange multipliers play a crucial role in the optimization process of Support Vector Machines (SVM), particularly in the formulation of the optimization problem that seeks to find the optimal hyperplane for classification. Here’s a detailed explanation of their role:\n",
    "\n",
    "1. Optimization Problem Formulation\n",
    "In SVM, the goal is to find a hyperplane that maximizes the margin between two classes while ensuring that the data points are correctly classified. The optimization problem can be stated as:\n",
    "\n",
    "[ \\text{Minimize} \\quad \\frac{1}{2} |\\mathbf{w}|^2 ]\n",
    "\n",
    "subject to the constraints:\n",
    "\n",
    "[ y_i (\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1 \\quad \\forall i ]\n",
    "\n",
    "where:\n",
    "\n",
    "(\\mathbf{w}) is the weight vector,\n",
    "(b) is the bias term,\n",
    "(y_i) is the label of the data point (\\mathbf{x}_i) (either +1 or -1).\n",
    "2. Introducing Lagrange Multipliers\n",
    "To solve this constrained optimization problem, we use the method of Lagrange multipliers. The idea is to convert the constrained problem into an unconstrained one by incorporating the constraints into the objective function using Lagrange multipliers.\n",
    "\n",
    "We introduce a Lagrange multiplier (\\alpha_i) for each constraint, leading to the Lagrangian function:\n",
    "\n",
    "[ \\mathcal{L}(\\mathbf{w}, b, \\boldsymbol{\\alpha}) = \\frac{1}{2} |\\mathbf{w}|^2 - \\sum_{i=1}^{N} \\alpha_i [y_i (\\mathbf{w} \\cdot \\mathbf{x}_i + b) - 1] ]\n",
    "\n",
    "where:\n",
    "\n",
    "(\\boldsymbol{\\alpha} = [\\alpha_1, \\alpha_2, \\ldots, \\alpha_N]) are the Lagrange multipliers.\n",
    "3. Dual Problem\n",
    "The next step is to find the stationary points of the Lagrangian by taking the partial derivatives with respect to (\\mathbf{w}) and (b) and setting them to zero:\n",
    "\n",
    "Gradient with respect to (\\mathbf{w}):\n",
    "[ \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}} = \\mathbf{w} - \\sum_{i=1}^{N} \\alpha_i y_i \\mathbf{x}_i = 0 ]\n",
    "\n",
    "Gradient with respect to (b):\n",
    "[ \\frac{\\partial \\mathcal{L}}{\\partial b} = -\\sum_{i=1}^{N} \\alpha_i y_i = 0 ]\n",
    "\n",
    "These conditions lead to a system of equations that can be solved to find the optimal (\\mathbf{w}) and (b).\n",
    "\n",
    "4. Formulating the Dual Problem\n",
    "By substituting the expressions for (\\mathbf{w}) and (b) back into the Lagrangian, we can derive the dual problem, which is often more computationally efficient to solve. The dual problem is given by:\n",
    "\n",
    "[ \\text{Maximize} \\quad W(\\boldsymbol{\\alpha}) = \\sum_{i=1}^{N} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_i \\alpha_j y_i y_j K(\\mathbf{x}_i, \\mathbf{x}_j) ]\n",
    "\n",
    "subject to the constraints:\n",
    "\n",
    "[ \\alpha_i \\geq 0 \\quad \\text{and} \\quad \\sum_{i=1}^{N} \\alpha_i y_i = 0 ]\n",
    "\n",
    "where (K(\\mathbf{x}_i, \\mathbf{x}_j)) is the kernel function, which allows for non-linear classification.\n",
    "\n",
    "5. Support Vectors and Lagrange Multipliers\n",
    "The Lagrange multipliers (\\alpha_i) have a direct interpretation in the context of SVM:\n",
    "\n",
    "Support Vectors: Only the data points that are support vectors (the points closest to the hyperplane) will have non-zero (\\alpha_i). For all other points, (\\alpha_i) will be zero. This means that the support vectors are the only points that influence the position of the hyperplane.\n",
    "\n",
    "Margin Calculation: The values of (\\alpha_i) also determine the contribution of each support vector to the decision boundary. The larger the (\\alpha_i), the more influence that particular support vector has on the position of the hyperplane"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788cca61-3534-44cf-94b5-3b2ed41bf79f",
   "metadata": {},
   "source": [
    "Q5>> What are Support Vectors in SVM4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d126384c-578d-4bfe-a59e-92a85b8a84fc",
   "metadata": {},
   "source": [
    "Support vectors are a fundamental concept in Support Vector Machines (SVM) and play a crucial role in the algorithm's ability to classify data points effectively. Here’s a detailed explanation of what support vectors are and their significance in SVM:\n",
    "\n",
    "Definition of Support Vectors\n",
    "Support vectors are the data points that are closest to the decision boundary (hyperplane) in the feature space. These points are critical because they directly influence the position and orientation of the hyperplane that separates different classes.\n",
    "\n",
    "Characteristics of Support Vectors\n",
    "Proximity to the Hyperplane: Support vectors lie on the edge of the margin, which is the region between the two classes. They are the points that are either on the margin or misclassified (in the case of soft margin SVM).\n",
    "\n",
    "Influence on the Decision Boundary: The decision boundary (hyperplane) is determined entirely by the support vectors. If you were to remove non-support vector points from the dataset, the position of the hyperplane would remain unchanged. However, removing support vectors would alter the hyperplane.\n",
    "\n",
    "Classification: In SVM, the classification of new data points is based on their position relative to the hyperplane defined by the support vectors. If a new point lies on the same side of the hyperplane as the support vectors of a particular class, it is classified as belonging to that class.\n",
    "\n",
    "Mathematical Representation\n",
    "In the context of SVM, the decision function can be expressed as:\n",
    "\n",
    "[ f(\\mathbf{x}) = \\text{sign}\\left(\\sum_{i=1}^{N} \\alpha_i y_i K(\\mathbf{x}_i, \\mathbf{x}) + b\\right) ]\n",
    "\n",
    "where:\n",
    "\n",
    "(\\alpha_i) are the Lagrange multipliers associated with the support vectors,\n",
    "(y_i) are the labels of the support vectors,\n",
    "(K(\\mathbf{x}_i, \\mathbf{x})) is the kernel function (which can be linear or non-linear),\n",
    "(b) is the bias term.\n",
    "Only the support vectors have non-zero (\\alpha_i), meaning they are the only points that contribute to the decision function.\n",
    "\n",
    "Importance of Support Vectors\n",
    "Model Efficiency: Support vectors allow SVM to be efficient in terms of memory and computation. The model is defined by a subset of the training data (the support vectors), which can significantly reduce the complexity of the model.\n",
    "\n",
    "Robustness: SVM is robust to overfitting, especially in high-dimensional spaces, because it focuses on the most informative data points (the support vectors) rather than all data points.\n",
    "\n",
    "Generalization: The presence of support vectors helps SVM generalize well to unseen data. Since the decision boundary is determined by the points that are most critical for classification, the model is less likely to be influenced by noise or outliers that do not affect the support vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6c8181-5d31-4d03-bc7b-a76dd5caa1a7",
   "metadata": {},
   "source": [
    "Q6>>What is a Support Vector Classifier (SVC)4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75824e78-c5e7-4d03-ba38-6750abdcca73",
   "metadata": {},
   "source": [
    "A Support Vector Classifier (SVC) is a specific implementation of the Support Vector Machine (SVM) algorithm that is used for classification tasks. It is designed to find the optimal hyperplane that separates data points of different classes in a high-dimensional space. Here’s a detailed overview of SVC, including its key features, working principles, and applications:\n",
    "\n",
    "Key Features of Support Vector Classifier (SVC)\n",
    "Supervised Learning: SVC is a supervised learning algorithm, meaning it requires labeled training data to learn the classification boundaries.\n",
    "\n",
    "Hyperplane: The primary goal of SVC is to identify the hyperplane that best separates the classes in the feature space. The hyperplane is defined by the weight vector (\\mathbf{w}) and the bias term (b).\n",
    "\n",
    "Support Vectors: SVC focuses on the support vectors, which are the data points closest to the hyperplane. These points are critical in determining the position and orientation of the hyperplane.\n",
    "\n",
    "Margin Maximization: SVC aims to maximize the margin, which is the distance between the hyperplane and the nearest data points from either class (the support vectors). A larger margin generally leads to better generalization on unseen data.\n",
    "\n",
    "Kernel Trick: SVC can handle both linear and non-linear classification problems. For non-linear cases, it employs the kernel trick, which allows the algorithm to operate in a higher-dimensional space without explicitly transforming the data. Common kernel functions include:\n",
    "\n",
    "Linear Kernel: Suitable for linearly separable data.\n",
    "Polynomial Kernel: Captures polynomial relationships between features.\n",
    "Radial Basis Function (RBF) Kernel: Effective for non-linear data, it measures the distance from a center point and can create complex decision boundaries.\n",
    "Soft Margin: SVC can incorporate a soft margin, allowing for some misclassifications. This is controlled by a regularization parameter (C):\n",
    "\n",
    "A small (C) value allows for a larger margin but permits more misclassifications.\n",
    "A large (C) value emphasizes correct classification, leading to a smaller margin.\n",
    "Mathematical Formulation\n",
    "The SVC optimization problem can be formulated as follows:\n",
    "\n",
    "Objective: Minimize the following function:\n",
    "[ \\text{Minimize} \\quad \\frac{1}{2} |\\mathbf{w}|^2 + C \\sum_{i=1}^{N} \\xi_i ]\n",
    "\n",
    "where (\\xi_i) are slack variables that allow for misclassifications.\n",
    "\n",
    "Constraints: Subject to the constraints:\n",
    "[ y_i (\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1 - \\xi_i \\quad \\forall i ]\n",
    "\n",
    "where (y_i) is the label of the data point (\\mathbf{x}_i) (either +1 or -1).\n",
    "\n",
    "Applications of SVC\n",
    "SVC is widely used in various fields due to its effectiveness and versatility. Some common applications include:\n",
    "\n",
    "Text Classification: SVC is often used for spam detection, sentiment analysis, and document categorization.\n",
    "\n",
    "Image Recognition: It can classify images based on features extracted from the images, such as in facial recognition or object detection.\n",
    "\n",
    "Bioinformatics: SVC is used for classifying genes, proteins, and other biological data.\n",
    "\n",
    "Finance: It can be applied in credit scoring, fraud detection, and risk assessment.\n",
    "\n",
    "Medical Diagnosis: SVC is used to classify medical images or patient data for disease diagnosis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e655545-22da-4c88-9bff-48a9a9fb3d33",
   "metadata": {},
   "source": [
    "Q7>>What is a Support Vector Regressor (SVR)4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f7a3fe-f02c-4689-9aea-323ab7fbd278",
   "metadata": {},
   "source": [
    "A Support Vector Regressor (SVR) is an extension of the Support Vector Machine (SVM) algorithm used for regression tasks. While SVM is primarily designed for classification, SVR adapts the principles of SVM to predict continuous values rather than discrete class labels. Here’s a detailed overview of SVR, including its key features, working principles, and applications:\n",
    "\n",
    "Key Features of Support Vector Regressor (SVR)\n",
    "Supervised Learning: SVR is a supervised learning algorithm, meaning it requires labeled training data to learn the relationships between input features and continuous output values.\n",
    "\n",
    "Hyperplane: In SVR, the goal is to find a hyperplane (or a function) that best fits the data points in a high-dimensional space. This hyperplane is defined by a weight vector (\\mathbf{w}) and a bias term (b).\n",
    "\n",
    "Epsilon-Insensitive Loss: SVR introduces the concept of an epsilon ((\\epsilon)) margin around the hyperplane. The idea is to ignore errors (deviations from the predicted values) that fall within this margin. This means that the model does not penalize errors that are smaller than (\\epsilon), allowing for a certain level of tolerance in predictions.\n",
    "\n",
    "Support Vectors: Similar to SVM, SVR focuses on support vectors, which are the data points that lie outside the epsilon margin. These points are critical in defining the regression function. The support vectors influence the position of the hyperplane, while points within the margin do not affect the model.\n",
    "\n",
    "Regularization: SVR includes a regularization parameter (C) that controls the trade-off between maximizing the margin and minimizing the prediction error. A larger (C) value places more emphasis on minimizing errors, while a smaller (C) value allows for a wider margin with more tolerance for errors.\n",
    "\n",
    "Kernel Trick: SVR can handle both linear and non-linear regression problems using the kernel trick. By applying kernel functions (such as linear, polynomial, or radial basis function (RBF) kernels), SVR can map the input data into a higher-dimensional space where a linear regression function can be applied.\n",
    "\n",
    "Mathematical Formulation\n",
    "The SVR optimization problem can be formulated as follows:\n",
    "\n",
    "Objective: Minimize the following function:\n",
    "[ \\text{Minimize} \\quad \\frac{1}{2} |\\mathbf{w}|^2 + C \\sum_{i=1}^{N} (\\xi_i + \\xi_i^*) ]\n",
    "\n",
    "where (\\xi_i) and (\\xi_i^*) are slack variables that allow for deviations from the epsilon margin.\n",
    "\n",
    "Constraints: Subject to the constraints:\n",
    "[ y_i - (\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\leq \\epsilon + \\xi_i ] [ (\\mathbf{w} \\cdot \\mathbf{x}_i + b) - y_i \\leq \\epsilon + \\xi_i^* ]\n",
    "\n",
    "where (y_i) is the actual target value for the input (\\mathbf{x}_i).\n",
    "\n",
    "Applications of SVR\n",
    "SVR is widely used in various fields due to its effectiveness in handling regression tasks. Some common applications include:\n",
    "\n",
    "Financial Forecasting: SVR can be used to predict stock prices, market trends, and economic indicators.\n",
    "\n",
    "Time Series Prediction: It is effective in forecasting future values based on historical data, such as sales forecasting or demand prediction.\n",
    "\n",
    "Engineering: SVR can be applied in modeling and predicting physical phenomena, such as stress-strain relationships in materials.\n",
    "\n",
    "Environmental Science: It is used for predicting environmental variables, such as air quality indices or temperature changes.\n",
    "\n",
    "Healthcare: SVR can be employed to predict patient outcomes based on various health metrics and historical data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00338dc6-1493-47fd-8215-1af7b7574d06",
   "metadata": {},
   "source": [
    "Q8>>What is the Kernel Trick in SVM4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b732c871-f308-4c7d-8bf0-4189ffe4147c",
   "metadata": {},
   "source": [
    "The kernel trick is a powerful technique used in Support Vector Machines (SVM) and other machine learning algorithms to enable them to operate in high-dimensional spaces without explicitly transforming the data into those spaces. This allows SVM to efficiently handle non-linear classification and regression tasks. Here’s a detailed explanation of the kernel trick, its purpose, and how it works:\n",
    "\n",
    "Purpose of the Kernel Trick\n",
    "Non-Linear Separation: Many real-world datasets are not linearly separable. The kernel trick allows SVM to find a hyperplane that can separate classes in a transformed feature space, even when the original data is not linearly separable.\n",
    "\n",
    "Computational Efficiency: Instead of explicitly mapping data points into a higher-dimensional space (which can be computationally expensive), the kernel trick allows the algorithm to compute the inner products of the data points in the higher-dimensional space directly. This avoids the need for the actual transformation, making the computation more efficient.\n",
    "\n",
    "How the Kernel Trick Works\n",
    "Feature Mapping: The kernel trick involves mapping the input data (\\mathbf{x}) into a higher-dimensional feature space (\\Phi(\\mathbf{x})) using a mapping function (\\Phi). This mapping can be complex and is often not explicitly defined.\n",
    "\n",
    "Kernel Function: Instead of computing the dot product in the transformed space, SVM uses a kernel function (K) that computes the dot product of the mapped data points in the original space. The kernel function is defined as:\n",
    "\n",
    "[ K(\\mathbf{x}_i, \\mathbf{x}_j) = \\Phi(\\mathbf{x}_i) \\cdot \\Phi(\\mathbf{x}_j) ]\n",
    "\n",
    "This allows SVM to work with the original input data without needing to compute the mapping explicitly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f910d69-765b-47f5-82cb-d3f6ec7c079a",
   "metadata": {},
   "source": [
    "Q9>>Compare Linear Kernel, Polynomial Kernel, and RBF Kernel:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c17d5e-2872-43b8-a21c-d1d1f8cf53ab",
   "metadata": {},
   "source": [
    "When using Support Vector Machines (SVM), the choice of kernel function is crucial as it determines how the algorithm interprets the data and constructs the decision boundary. Here’s a comparison of three commonly used kernel functions: Linear Kernel, Polynomial Kernel, and Radial Basis Function (RBF) Kernel.\n",
    "\n",
    "1. Linear Kernel\n",
    "Definition:\n",
    "\n",
    "The linear kernel is the simplest kernel function. It computes the dot product of the input vectors directly.\n",
    "Formula: [ K(\\mathbf{x}_i, \\mathbf{x}_j) = \\mathbf{x}_i \\cdot \\mathbf{x}_j ]\n",
    "\n",
    "Characteristics:\n",
    "\n",
    "Linearly Separable Data: Best suited for datasets that are linearly separable.\n",
    "No Transformation: Does not transform the data into a higher-dimensional space; it operates in the original feature space.\n",
    "Computational Efficiency: Fast to compute, as it involves simple dot products.\n",
    "Use Cases:\n",
    "\n",
    "Text classification (e.g., spam detection) where features are often linearly separable.\n",
    "Situations where the number of features is large compared to the number of samples.\n",
    "2. Polynomial Kernel\n",
    "Definition:\n",
    "\n",
    "The polynomial kernel computes the dot product of the input vectors raised to a specified power, allowing for polynomial decision boundaries.\n",
    "Formula: [ K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\mathbf{x}_i \\cdot \\mathbf{x}_j + c)^d ] where (c) is a constant (often set to 1) and (d) is the degree of the polynomial.\n",
    "\n",
    "Characteristics:\n",
    "\n",
    "Non-Linear Decision Boundaries: Can create complex, non-linear decision boundaries based on the degree (d).\n",
    "Higher Complexity: As (d) increases, the model becomes more complex and can fit more intricate patterns in the data.\n",
    "Overfitting Risk: Higher degrees can lead to overfitting, especially with limited data.\n",
    "Use Cases:\n",
    "\n",
    "Situations where relationships between features are polynomial in nature.\n",
    "Applications in image recognition and other domains where non-linear relationships are expected.\n",
    "3. Radial Basis Function (RBF) Kernel\n",
    "Definition:\n",
    "\n",
    "The RBF kernel is a popular choice for non-linear classification. It measures the distance between data points and applies a Gaussian function.\n",
    "Formula: [ K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp\\left(-\\frac{|\\mathbf{x}_i - \\mathbf{x}_j|^2}{2\\sigma^2}\\right) ] where (\\sigma) is a parameter that controls the width of the Gaussian.\n",
    "\n",
    "Characteristics:\n",
    "\n",
    "Highly Flexible: Can create very complex decision boundaries, making it suitable for a wide range of data distributions.\n",
    "Local Influence: The influence of a training example decreases with distance, allowing the model to focus on nearby points.\n",
    "Parameter Sensitivity: The choice of (\\sigma) is crucial; a small (\\sigma) can lead to overfitting, while a large (\\sigma) can lead to underfitting.\n",
    "Use Cases:\n",
    "\n",
    "General-purpose kernel for many types of data, especially when the relationship between features is complex and non-linear.\n",
    "Applications in bioinformatics, image classification, and other domains where data is not linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159487ee-84b2-4476-9311-5befafc7f470",
   "metadata": {},
   "source": [
    "Q10>>What is the effect of the C parameter in SVM4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70e058c-848c-4bdd-a104-d7dbbbcb043a",
   "metadata": {},
   "source": [
    "The (C) parameter in Support Vector Machines (SVM) plays a crucial role in controlling the trade-off between maximizing the margin and minimizing classification errors. It is a regularization parameter that influences the behavior of the SVM model, particularly in the context of soft margin SVM. Here’s a detailed explanation of the effect of the (C) parameter:\n",
    "\n",
    "1. Understanding the Role of (C)\n",
    "Soft Margin SVM: In scenarios where the data is not perfectly separable, SVM allows for some misclassifications through the concept of a soft margin. The (C) parameter controls how much misclassification is tolerated.\n",
    "\n",
    "Objective Function: The optimization problem in soft margin SVM can be expressed as:\n",
    "\n",
    "[ \\text{Minimize} \\quad \\frac{1}{2} |\\mathbf{w}|^2 + C \\sum_{i=1}^{N} \\xi_i ]\n",
    "\n",
    "where:\n",
    "\n",
    "(|\\mathbf{w}|^2) represents the margin maximization term.\n",
    "(\\xi_i) are slack variables that allow for misclassifications.\n",
    "2. Effects of Different Values of (C)\n",
    "Large (C) Value:\n",
    "\n",
    "Emphasis on Correct Classification: A larger (C) value places a higher penalty on misclassifications. The model will prioritize correctly classifying all training points, even if it means sacrificing the margin.\n",
    "Narrow Margin: The decision boundary may become more complex and narrow, as the model tries to fit the training data closely.\n",
    "Risk of Overfitting: With a large (C), the model may become too sensitive to noise and outliers in the training data, leading to overfitting. This means the model performs well on the training data but poorly on unseen data.\n",
    "Small (C) Value:\n",
    "\n",
    "Emphasis on Margin Maximization: A smaller (C) value allows for a wider margin, accepting some misclassifications in favor of a simpler model.\n",
    "Broader Margin: The decision boundary may be smoother and less complex, as the model focuses on generalizing rather than fitting every training point.\n",
    "Risk of Underfitting: If (C) is too small, the model may ignore important patterns in the data, leading to underfitting. This means the model may not capture the underlying structure of the data well.\n",
    "3. Visualizing the Effect of (C)\n",
    "Large (C): The decision boundary will be tightly fitted around the training data, with fewer misclassifications but potentially a more complex shape.\n",
    "Small (C): The decision boundary will be more generalized, allowing for some misclassifications but potentially capturing the overall trend of the data better.\n",
    "4. Choosing the Right (C)\n",
    "Cross-Validation: The optimal value of (C) is often determined through cross-validation. By evaluating the model's performance on a validation set for different values of (C), one can select the value that provides the best balance between bias and variance.\n",
    "\n",
    "Domain Knowledge: Understanding the nature of the data and the problem at hand can also guide the choice of (C). For example, in noisy datasets, a smaller (C) might be preferable to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1147b639-e267-401c-87d7-a21e2ed77484",
   "metadata": {},
   "source": [
    "Q11>>What is the role of the Gamma parameter in RBF Kernel SVM4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa888d65-5eff-428a-937b-3aeeb1d7b540",
   "metadata": {},
   "source": [
    "The gamma parameter in the Radial Basis Function (RBF) kernel of Support Vector Machines (SVM) plays a crucial role in defining the shape and complexity of the decision boundary. It controls the influence of individual training examples on the decision boundary and affects how the model generalizes to unseen data. Here’s a detailed explanation of the role of the gamma parameter:\n",
    "\n",
    "1. Understanding Gamma in RBF Kernel\n",
    "The RBF kernel is defined as:\n",
    "\n",
    "[ K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp\\left(-\\frac{|\\mathbf{x}_i - \\mathbf{x}_j|^2}{2\\sigma^2}\\right) ]\n",
    "\n",
    "where (\\sigma) is a parameter that controls the width of the Gaussian function. However, in many implementations, gamma ((\\gamma)) is used instead of (\\sigma) and is defined as:\n",
    "\n",
    "[ \\gamma = \\frac{1}{2\\sigma^2} ]\n",
    "\n",
    "This means that a higher value of gamma corresponds to a smaller value of (\\sigma) and vice versa.\n",
    "\n",
    "2. Effects of Different Values of Gamma\n",
    "Large Gamma Value:\n",
    "\n",
    "Narrow Influence: A large gamma value means that the influence of each training example is limited to a small region around it. This results in a very tight decision boundary that closely follows the training data.\n",
    "Complex Decision Boundary: The model can capture intricate patterns in the data, leading to a highly complex decision boundary.\n",
    "Risk of Overfitting: With a large gamma, the model may fit the training data too closely, capturing noise and outliers, which can lead to overfitting. This means the model performs well on the training data but poorly on unseen data.\n",
    "Small Gamma Value:\n",
    "\n",
    "Wide Influence: A small gamma value means that the influence of each training example extends over a larger area. This results in a smoother and broader decision boundary.\n",
    "Simpler Decision Boundary: The model may not capture all the complexities of the data, leading to a more generalized decision boundary.\n",
    "Risk of Underfitting: If gamma is too small, the model may fail to capture important patterns in the data, leading to underfitting. This means the model may not perform well even on the training data.\n",
    "3. Visualizing the Effect of Gamma\n",
    "Large Gamma: The decision boundary will be very complex, potentially zigzagging around the training points. This can lead to a model that is very sensitive to the training data.\n",
    "Small Gamma: The decision boundary will be smoother and more generalized, potentially missing some of the finer details in the data.\n",
    "4. Choosing the Right Gamma\n",
    "Cross-Validation: The optimal value of gamma is often determined through cross-validation. By evaluating the model's performance on a validation set for different values of gamma, one can select the value that provides the best balance between bias and variance.\n",
    "\n",
    "Grid Search: A common approach is to perform a grid search over a range of values for both (C) (the regularization parameter) and (\\gamma) to find the combination that yields the best performance.\n",
    "\n",
    "Domain Knowledge: Understanding the nature of the data can also help in selecting an appropriate gamma value. For example, if the data is known to have complex relationships, a higher gamma might be more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c43e25f-f3b7-4c1c-85e4-4566430f20cb",
   "metadata": {},
   "source": [
    "Q12>>What is the Naïve Bayes classifier, and why is it called \"Naïve\"4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55d9377-4c01-423e-a91e-36006dcca582",
   "metadata": {},
   "source": [
    "The Naïve Bayes classifier is a family of probabilistic algorithms based on Bayes' theorem, used for classification tasks. It is particularly popular for text classification, spam detection, and sentiment analysis due to its simplicity and effectiveness. Here’s a detailed overview of the Naïve Bayes classifier, including its principles, types, and the reason behind its \"naïve\" designation.\n",
    "\n",
    "1. Bayes' Theorem\n",
    "At the core of the Naïve Bayes classifier is Bayes' theorem, which describes the probability of a class given some features. The theorem is expressed as:\n",
    "\n",
    "[ P(C | X) = \\frac{P(X | C) \\cdot P(C)}{P(X)} ]\n",
    "\n",
    "Where:\n",
    "\n",
    "(P(C | X)) is the posterior probability of class (C) given the features (X).\n",
    "(P(X | C)) is the likelihood of the features (X) given class (C).\n",
    "(P(C)) is the prior probability of class (C).\n",
    "(P(X)) is the total probability of the features (X).\n",
    "2. Naïve Assumption\n",
    "The term \"naïve\" refers to the assumption made by the classifier that all features are independent of each other given the class label. This assumption simplifies the computation of the likelihood (P(X | C)) as follows:\n",
    "\n",
    "[ P(X | C) = P(x_1 | C) \\cdot P(x_2 | C) \\cdot \\ldots \\cdot P(x_n | C) ]\n",
    "\n",
    "Where (x_1, x_2, \\ldots, x_n) are the individual features. This means that the classifier assumes that the presence (or absence) of a particular feature does not affect the presence (or absence) of any other feature, which is often not true in real-world data.\n",
    "\n",
    "3. Types of Naïve Bayes Classifiers\n",
    "There are several types of Naïve Bayes classifiers, depending on the nature of the features:\n",
    "\n",
    "Gaussian Naïve Bayes: Assumes that the features follow a Gaussian (normal) distribution. It is suitable for continuous data.\n",
    "\n",
    "Multinomial Naïve Bayes: Suitable for discrete data, particularly for text classification where the features are the counts of words or tokens.\n",
    "\n",
    "Bernoulli Naïve Bayes: Similar to the multinomial variant but assumes binary features (i.e., whether a feature is present or absent).\n",
    "\n",
    "4. Advantages of Naïve Bayes Classifier\n",
    "Simplicity: The algorithm is easy to implement and understand.\n",
    "Efficiency: It is computationally efficient, requiring a small amount of training data to estimate the parameters.\n",
    "Performance: Despite its simplicity and the naive assumption of feature independence, it often performs surprisingly well in practice, especially for text classification tasks.\n",
    "5. Disadvantages of Naïve Bayes Classifier\n",
    "Independence Assumption: The assumption that features are independent is often unrealistic, which can lead to suboptimal performance in some cases.\n",
    "Zero Probability Problem: If a particular feature value does not occur in the training data for a given class, the model will assign a probability of zero to that class for new instances. This can be mitigated using techniques like Laplace smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c20fbf-a775-4a8c-b83a-574857c5ee14",
   "metadata": {},
   "source": [
    "Q13>>What is Bayes’ Theorem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23d3b66-777f-420f-9cf0-5dd1dbc1918e",
   "metadata": {},
   "source": [
    "Bayes' Theorem is a fundamental concept in probability theory and statistics that describes how to update the probability of a hypothesis based on new evidence. It provides a mathematical framework for reasoning about uncertainty and is widely used in various fields, including statistics, machine learning, medicine, and finance.\n",
    "\n",
    "The Formula\n",
    "Bayes' Theorem is expressed mathematically as:\n",
    "\n",
    "[ P(A | B) = \\frac{P(B | A) \\cdot P(A)}{P(B)} ]\n",
    "\n",
    "Where:\n",
    "\n",
    "(P(A | B)) is the posterior probability: the probability of event (A) occurring given that (B) is true.\n",
    "(P(B | A)) is the likelihood: the probability of event (B) occurring given that (A) is true.\n",
    "(P(A)) is the prior probability: the initial probability of event (A) occurring before observing (B).\n",
    "(P(B)) is the marginal probability: the total probability of event (B) occurring under all possible scenarios.\n",
    "Explanation of Terms\n",
    "Prior Probability ((P(A))): This represents what is known about the hypothesis (A) before considering the new evidence (B). It reflects the initial belief about the hypothesis.\n",
    "\n",
    "Likelihood ((P(B | A))): This measures how likely the evidence (B) is, assuming that the hypothesis (A) is true. It quantifies the strength of the evidence in favor of the hypothesis.\n",
    "\n",
    "Marginal Probability ((P(B))): This is the total probability of observing the evidence (B) across all possible hypotheses. It can be calculated using the law of total probability:\n",
    "\n",
    "[ P(B) = P(B | A) \\cdot P(A) + P(B | \\neg A) \\cdot P(\\neg A) ]\n",
    "\n",
    "where (\\neg A) represents the complement of (A).\n",
    "\n",
    "Posterior Probability ((P(A | B))): This is the updated probability of the hypothesis (A) after taking into account the new evidence (B). It reflects the revised belief about the hypothesis based on the evidence.\n",
    "Intuition Behind Bayes' Theorem\n",
    "Bayes' Theorem allows us to update our beliefs in light of new evidence. For example, if we have a prior belief about the likelihood of a disease (hypothesis (A)), and we receive a positive test result (evidence (B)), Bayes' Theorem helps us calculate the probability that the person actually has the disease given the positive test result.\n",
    "\n",
    "Applications of Bayes' Theorem\n",
    "Bayes' Theorem has numerous applications, including:\n",
    "\n",
    "Medical Diagnosis: Updating the probability of a disease based on test results.\n",
    "Spam Filtering: Classifying emails as spam or not spam based on the presence of certain words.\n",
    "Machine Learning: Used in algorithms like Naïve Bayes classifiers, which apply Bayes' Theorem with the assumption of feature independence.\n",
    "Risk Assessment: Evaluating the likelihood of various outcomes based on prior knowledge and new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ac61c3-cfc8-4b3e-b150-1029658502fb",
   "metadata": {},
   "source": [
    "Q14>>Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b6f9bc-caa8-4198-8f9b-3bca10faca4c",
   "metadata": {},
   "source": [
    "Naïve Bayes classifiers are a family of probabilistic algorithms based on Bayes' theorem, and they are particularly useful for classification tasks. There are several variants of Naïve Bayes classifiers, each suited for different types of data. The three most common types are Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes. Here’s a detailed comparison of these three variants:\n",
    "\n",
    "1. Gaussian Naïve Bayes\n",
    "Definition:\n",
    "\n",
    "Gaussian Naïve Bayes assumes that the features follow a Gaussian (normal) distribution. It is suitable for continuous data.\n",
    "Key Characteristics:\n",
    "\n",
    "Continuous Features: It is used when the features are continuous and can take any real value.\n",
    "\n",
    "Probability Density Function: The likelihood of the features is calculated using the probability density function of the Gaussian distribution:\n",
    "\n",
    "[ P(x_i | C) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right) ]\n",
    "\n",
    "where (\\mu) is the mean and (\\sigma^2) is the variance of the feature for class (C).\n",
    "\n",
    "Assumption of Independence: Like all Naïve Bayes classifiers, it assumes that the features are conditionally independent given the class label.\n",
    "\n",
    "Use Cases:\n",
    "\n",
    "Suitable for datasets where the features are continuous and normally distributed, such as in some medical or financial applications.\n",
    "2. Multinomial Naïve Bayes\n",
    "Definition:\n",
    "\n",
    "Multinomial Naïve Bayes is designed for discrete data, particularly for text classification tasks where the features represent the frequency of words or tokens.\n",
    "Key Characteristics:\n",
    "\n",
    "Discrete Features: It is used when the features are counts or frequencies, such as the number of times a word appears in a document.\n",
    "\n",
    "Probability Mass Function: The likelihood of the features is calculated using the multinomial distribution:\n",
    "\n",
    "[ P(x_i | C) = \\frac{(n_i)!}{(n_i^{(1)})!(n_i^{(2)})! \\ldots (n_i^{(k)})!} \\cdot \\prod_{j=1}^{k} p_j^{n_i^{(j)}} ]\n",
    "\n",
    "where (n_i) is the total count of features for class (C), and (p_j) is the probability of feature (j) given class (C).\n",
    "\n",
    "Assumption of Independence: It also assumes that the features are conditionally independent given the class label.\n",
    "\n",
    "Use Cases:\n",
    "\n",
    "Commonly used in text classification tasks, such as spam detection and sentiment analysis, where the input features are word counts or term frequencies.\n",
    "3. Bernoulli Naïve Bayes\n",
    "Definition:\n",
    "\n",
    "Bernoulli Naïve Bayes is similar to Multinomial Naïve Bayes but is specifically designed for binary/boolean features. It assumes that each feature is a binary indicator (i.e., whether a feature is present or absent).\n",
    "Key Characteristics:\n",
    "\n",
    "Binary Features: It is used when the features are binary (0 or 1), indicating the presence or absence of a feature.\n",
    "\n",
    "Probability Mass Function: The likelihood of the features is calculated using the Bernoulli distribution:\n",
    "\n",
    "[ P(x_i | C) = p^{x_i} (1 - p)^{(1 - x_i)} ]\n",
    "\n",
    "where (p) is the probability of the feature being present given class (C).\n",
    "\n",
    "Assumption of Independence: Like the other variants, it assumes that the features are conditionally independent given the class label.\n",
    "\n",
    "Use Cases:\n",
    "\n",
    "Suitable for text classification tasks where the presence or absence of words is more relevant than their frequency, such as in document classification where binary features are used.\n",
    "Summary of Differences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236a130d-677e-4f5c-88f5-3bfd22836121",
   "metadata": {},
   "source": [
    "Q15>>When should you use Gaussian Naïve Bayes over other variants4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45548ec4-6a3b-46ac-aa8c-6003de4d18fc",
   "metadata": {},
   "source": [
    "Gaussian Naïve Bayes is a specific variant of the Naïve Bayes classifier that assumes the features follow a Gaussian (normal) distribution. Here are some scenarios and considerations for when you should use Gaussian Naïve Bayes over other variants like Multinomial Naïve Bayes or Bernoulli Naïve Bayes:\n",
    "\n",
    "1. Continuous Features\n",
    "Nature of Data: Use Gaussian Naïve Bayes when your dataset consists of continuous features. This is particularly relevant in cases where the features can take any real value, such as measurements (e.g., height, weight, temperature) or other continuous variables.\n",
    "\n",
    "Normal Distribution: If you have prior knowledge or empirical evidence that the continuous features are normally distributed, Gaussian Naïve Bayes is a suitable choice. You can visualize the distribution of your features using histograms or Q-Q plots to check for normality.\n",
    "\n",
    "2. Simplicity and Speed\n",
    "Computational Efficiency: Gaussian Naïve Bayes is computationally efficient and easy to implement. If you need a quick and straightforward model for classification, especially in exploratory data analysis or when working with large datasets, Gaussian Naïve Bayes can be a good option.\n",
    "3. Baseline Model\n",
    "Initial Benchmarking: Gaussian Naïve Bayes can serve as a good baseline model for classification tasks. It provides a simple and interpretable model that can be used to compare against more complex models. If Gaussian Naïve Bayes performs well, it may be sufficient for your needs.\n",
    "4. Handling Multicollinearity\n",
    "Independence Assumption: While Gaussian Naïve Bayes assumes that features are conditionally independent given the class label, it can still perform reasonably well even when this assumption is violated to some extent. If you have multicollinear features (features that are correlated), Gaussian Naïve Bayes can still be effective, especially if the features are normally distributed.\n",
    "5. Small Sample Sizes\n",
    "Limited Data: In situations where you have a small sample size, Gaussian Naïve Bayes can be advantageous because it requires fewer parameters to estimate compared to more complex models. This can help prevent overfitting when data is scarce.\n",
    "6. Interpretability\n",
    "Model Interpretability: Gaussian Naïve Bayes provides a clear probabilistic interpretation of the predictions. If interpretability is important for your application (e.g., in medical diagnosis), Gaussian Naïve Bayes can be a suitable choice.\n",
    "Summary\n",
    "In summary, you should consider using Gaussian Naïve Bayes when:\n",
    "\n",
    "Your dataset consists of continuous features that are likely to follow a normal distribution.\n",
    "You need a simple, efficient, and interpretable model for classification.\n",
    "You want to establish a baseline model for comparison with more complex classifiers.\n",
    "You are dealing with small sample sizes or multicollinearity among features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2291e98d-1049-456c-9e2e-bc1048f4d2a3",
   "metadata": {},
   "source": [
    "Q16>>What are the key assumptions made by Naïve Bayes4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830f5bff-1392-4e9f-94d7-29d271f5d0ae",
   "metadata": {},
   "source": [
    "Naïve Bayes classifiers are based on Bayes' theorem and make several key assumptions that simplify the computation of probabilities. These assumptions are crucial for the functioning of the algorithm and influence its performance. Here are the primary assumptions made by Naïve Bayes:\n",
    "\n",
    "1. Conditional Independence Assumption\n",
    "Independence of Features: The most significant assumption of Naïve Bayes is that all features (or attributes) are conditionally independent given the class label. This means that the presence (or absence) of a particular feature does not affect the presence (or absence) of any other feature when the class label is known.\n",
    "\n",
    "Mathematically, this can be expressed as:\n",
    "\n",
    "[ P(X_1, X_2, \\ldots, X_n | C) = P(X_1 | C) \\cdot P(X_2 | C) \\cdot \\ldots \\cdot P(X_n | C) ]\n",
    "\n",
    "where (X_1, X_2, \\ldots, X_n) are the features and (C) is the class label.\n",
    "\n",
    "Implication: This assumption simplifies the computation of the joint probability of the features given the class label, allowing the model to be trained efficiently. However, in practice, this assumption may not hold true, especially in datasets where features are correlated.\n",
    "\n",
    "2. Feature Distribution Assumption\n",
    "Distribution of Features: Different variants of Naïve Bayes make specific assumptions about the distribution of the features:\n",
    "\n",
    "Gaussian Naïve Bayes: Assumes that the continuous features follow a Gaussian (normal) distribution.\n",
    "Multinomial Naïve Bayes: Assumes that the features represent counts or frequencies and follows a multinomial distribution.\n",
    "Bernoulli Naïve Bayes: Assumes that the features are binary (0 or 1) and follows a Bernoulli distribution.\n",
    "Implication: These distribution assumptions affect how the likelihood of the features is calculated and can impact the model's performance if the actual data distribution deviates significantly from these assumptions.\n",
    "\n",
    "3. Prior Probability Assumption\n",
    "Prior Independence: Naïve Bayes assumes that the prior probabilities of the classes are independent of the features. The prior probability (P(C)) is estimated from the training data based on the frequency of each class.\n",
    "\n",
    "Implication: This assumption allows the model to compute the posterior probability of a class given the features using Bayes' theorem. However, it also means that the model does not account for any potential dependencies between the class prior and the features.\n",
    "\n",
    "4. Simplicity and Interpretability\n",
    "Model Simplicity: Naïve Bayes is designed to be a simple and interpretable model. The assumptions made lead to a straightforward implementation and fast computation, making it suitable for large datasets and real-time applications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19447f80-82e7-488c-9d14-1c58f0e19315",
   "metadata": {},
   "source": [
    "Q16>>What are the advantages and disadvantages of Naïve Bayes4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1115d02d-ef7b-4010-9e44-d135c35b4000",
   "metadata": {},
   "source": [
    "Naïve Bayes classifiers are widely used in various applications due to their simplicity and effectiveness. However, like any machine learning algorithm, they come with their own set of advantages and disadvantages. Here’s a detailed overview:\n",
    "\n",
    "Advantages of Naïve Bayes\n",
    "Simplicity and Ease of Implementation:\n",
    "\n",
    "Naïve Bayes is straightforward to understand and implement. The underlying mathematics is relatively simple, making it accessible for beginners in machine learning.\n",
    "Fast Training and Prediction:\n",
    "\n",
    "The algorithm is computationally efficient, requiring only a single pass through the training data to estimate the probabilities. This results in fast training times, making it suitable for large datasets.\n",
    "Works Well with High-Dimensional Data:\n",
    "\n",
    "Naïve Bayes performs well in high-dimensional spaces, such as text classification tasks (e.g., spam detection, sentiment analysis) where the number of features (words) can be very large.\n",
    "Robustness to Irrelevant Features:\n",
    "\n",
    "The model can handle irrelevant features reasonably well. Since it treats each feature independently, the presence of irrelevant features does not significantly affect the performance.\n",
    "Good Performance with Small Datasets:\n",
    "\n",
    "Naïve Bayes can perform surprisingly well even with small amounts of training data, especially when the independence assumptions hold true.\n",
    "Probabilistic Output:\n",
    "\n",
    "The algorithm provides probabilistic predictions, which can be useful for applications where understanding the confidence of predictions is important.\n",
    "Disadvantages of Naïve Bayes\n",
    "Conditional Independence Assumption:\n",
    "\n",
    "The most significant limitation of Naïve Bayes is the assumption that all features are conditionally independent given the class label. In practice, this assumption often does not hold, especially in datasets where features are correlated, which can lead to suboptimal performance.\n",
    "Feature Distribution Assumptions:\n",
    "\n",
    "Different variants of Naïve Bayes make specific assumptions about the distribution of features (e.g., Gaussian for continuous features, multinomial for counts). If the actual data distribution deviates significantly from these assumptions, the model's performance may suffer.\n",
    "Zero Probability Problem:\n",
    "\n",
    "If a particular feature value does not occur in the training data for a given class, the model will assign a probability of zero to that class for new instances. This can be mitigated using techniques like Laplace smoothing, but it remains a concern.\n",
    "Limited Expressiveness:\n",
    "\n",
    "Naïve Bayes is a linear classifier, which means it may struggle with complex decision boundaries. It may not perform well on datasets that require more sophisticated modeling of relationships between features.\n",
    "Sensitivity to Imbalanced Data:\n",
    "\n",
    "Naïve Bayes can be sensitive to class imbalances, where one class significantly outnumbers another. This can lead to biased predictions favoring the majority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1358dd1a-bfbe-4227-846c-de63de70d91c",
   "metadata": {},
   "source": [
    "Q18>>Why is Naïve Bayes a good choice for text classification4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f578b48-6765-4695-afb4-3cc42f6c0eda",
   "metadata": {},
   "source": [
    "1. Simplicity and Efficiency\n",
    "Easy to Implement: The algorithm is straightforward to understand and implement, making it accessible for practitioners and researchers.\n",
    "Fast Training and Prediction: Naïve Bayes classifiers are computationally efficient, requiring only a single pass through the training data to estimate probabilities. This results in quick training and prediction times, which is beneficial for large text datasets.\n",
    "2. High Dimensionality Handling\n",
    "Effective in High-Dimensional Spaces: Text data often consists of a large number of features (e.g., words or tokens). Naïve Bayes can handle high-dimensional data effectively, as it does not require complex computations for each feature.\n",
    "Feature Independence: The conditional independence assumption allows Naïve Bayes to treat each word as an independent feature, simplifying the calculations and making it feasible to work with large vocabularies.\n",
    "3. Robustness to Irrelevant Features\n",
    "Tolerance for Noise: In text classification, many features (words) may be irrelevant or noisy. Naïve Bayes is robust to irrelevant features because it treats each feature independently. The presence of irrelevant words does not significantly impact the overall classification performance.\n",
    "4. Probabilistic Output\n",
    "Confidence Scores: Naïve Bayes provides probabilistic predictions, allowing users to understand the confidence level of the classifications. This can be particularly useful in applications where the certainty of a prediction is important, such as in medical diagnosis or risk assessment.\n",
    "5. Good Performance with Small Datasets\n",
    "Effective with Limited Data: Naïve Bayes can perform well even with relatively small amounts of training data, which is often the case in text classification tasks where labeled data may be scarce.\n",
    "6. Works Well with Bag-of-Words Model\n",
    "Compatibility with Text Representation: Naïve Bayes is well-suited for the bag-of-words model, where text is represented as a collection of word counts or frequencies. The multinomial variant of Naïve Bayes is particularly effective in this context, as it directly models the frequency of words in documents.\n",
    "7. Baseline Model\n",
    "Benchmarking: Naïve Bayes is often used as a baseline model in text classification tasks. Its simplicity allows it to serve as a reference point against which more complex models can be compared. If Naïve Bayes performs well, it may be sufficient for the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3524fd7c-dbdd-4bbf-b2d7-de6b238e70ca",
   "metadata": {},
   "source": [
    "Q19>>Compare SVM and Naïve Bayes for classification tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc746822-9e56-4742-b759-15b3d909c0d7",
   "metadata": {},
   "source": [
    "Support Vector Machines (SVM) and Naïve Bayes are both popular classification algorithms used in machine learning, but they have different underlying principles, strengths, and weaknesses. Here’s a detailed comparison of SVM and Naïve Bayes for classification tasks:\n",
    "\n",
    "1. Algorithm Type\n",
    "SVM:\n",
    "SVM is a discriminative classifier that aims to find the optimal hyperplane that separates different classes in the feature space. It focuses on maximizing the margin between the classes.\n",
    "Naïve Bayes:\n",
    "Naïve Bayes is a probabilistic classifier based on Bayes' theorem. It calculates the posterior probability of each class given the features and makes predictions based on the class with the highest probability.\n",
    "2. Assumptions\n",
    "SVM:\n",
    "\n",
    "SVM does not make strong assumptions about the distribution of the data. It can handle non-linear relationships through the use of kernel functions, allowing it to create complex decision boundaries.\n",
    "Naïve Bayes:\n",
    "\n",
    "Naïve Bayes makes the strong assumption of conditional independence among features given the class label. This means it assumes that the presence of one feature does not affect the presence of another feature, which may not hold true in many real-world datasets.\n",
    "3. Feature Types\n",
    "SVM:\n",
    "\n",
    "SVM can handle both linear and non-linear data and is effective for high-dimensional feature spaces. It works well with continuous and categorical features.\n",
    "Naïve Bayes:\n",
    "\n",
    "Naïve Bayes has different variants tailored for specific types of data:\n",
    "Gaussian Naïve Bayes: Assumes continuous features follow a Gaussian distribution.\n",
    "Multinomial Naïve Bayes: Suitable for discrete features, particularly for text classification (word counts).\n",
    "Bernoulli Naïve Bayes: Suitable for binary features (presence/absence).\n",
    "4. Performance with Small Datasets\n",
    "SVM:\n",
    "\n",
    "SVM can perform well with small to medium-sized datasets, but it may require careful tuning of hyperparameters (like (C) and (\\gamma)) to avoid overfitting.\n",
    "Naïve Bayes:\n",
    "\n",
    "Naïve Bayes can perform surprisingly well with small datasets, especially when the independence assumptions hold true. It is less prone to overfitting due to its simplicity.\n",
    "5. Computational Efficiency\n",
    "SVM:\n",
    "\n",
    "SVM can be computationally intensive, especially with large datasets and complex kernels. Training time can increase significantly with the number of samples and features.\n",
    "Naïve Bayes:\n",
    "\n",
    "Naïve Bayes is computationally efficient, requiring only a single pass through the training data to estimate probabilities. It is generally faster for both training and prediction.\n",
    "6. Interpretability\n",
    "SVM:\n",
    "\n",
    "SVM models can be less interpretable, especially when using non-linear kernels. Understanding the decision boundary and the role of individual features can be challenging.\n",
    "Naïve Bayes:\n",
    "\n",
    "Naïve Bayes is more interpretable, as it provides probabilistic outputs and clear insights into how features contribute to the classification.\n",
    "7. Handling Imbalanced Data\n",
    "SVM:\n",
    "\n",
    "SVM can be sensitive to class imbalances, as it tries to maximize the margin. Techniques like class weighting can be used to address this issue.\n",
    "Naïve Bayes:\n",
    "\n",
    "Naïve Bayes can also be affected by imbalanced classes, but it can handle this better in some cases due to its probabilistic nature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e568ac11-4681-45b7-91c0-a51643fd7a12",
   "metadata": {},
   "source": [
    "Q20>>How does Laplace Smoothing help in Naïve Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d5222b-c5b9-4f31-8d04-f9b4c8f2f3d5",
   "metadata": {},
   "source": [
    "Laplace smoothing, also known as additive smoothing, is a technique used in Naïve Bayes classifiers to handle the problem of zero probabilities when estimating the likelihood of features given a class. This is particularly important in scenarios where certain feature values do not appear in the training data for a given class, which can lead to issues in classification. Here’s how Laplace smoothing helps in Naïve Bayes:\n",
    "\n",
    "1. Problem of Zero Probability\n",
    "In Naïve Bayes, the likelihood of a feature given a class is calculated based on the frequency of that feature in the training data. If a particular feature value does not occur in the training data for a specific class, the probability of that feature given the class will be zero. This can lead to the following issues:\n",
    "\n",
    "Zero Probability Issue: If any feature has a zero probability for a class, the entire product of probabilities for that class will also be zero. This means that the model will not consider that class at all when making predictions, which can be problematic, especially in text classification tasks where certain words may not appear in every class.\n",
    "2. Laplace Smoothing Technique\n",
    "Laplace smoothing addresses the zero probability problem by adding a small constant (usually 1) to the count of each feature occurrence. This ensures that no feature has a zero probability. The formula for calculating the smoothed probability of a feature given a class is as follows:\n",
    "\n",
    "[ P(x_i | C) = \\frac{N_{i,j} + 1}{N_{j} + V} ]\n",
    "\n",
    "Where:\n",
    "\n",
    "(N_{i,j}) is the count of feature (x_i) in class (C).\n",
    "(N_{j}) is the total count of all features in class (C).\n",
    "(V) is the total number of unique features (vocabulary size).\n",
    "3. Benefits of Laplace Smoothing\n",
    "Avoids Zero Probabilities: By adding 1 to the count of each feature, Laplace smoothing ensures that no feature has a zero probability, allowing the model to make predictions even when certain features are absent in the training data.\n",
    "\n",
    "Improves Generalization: Laplace smoothing helps the model generalize better to unseen data by preventing it from being overly confident in its predictions based on limited training data.\n",
    "\n",
    "Stabilizes Estimates: It stabilizes the probability estimates, especially in cases where the training data is sparse or imbalanced. This is particularly important in text classification, where certain words may appear frequently in one class but not at all in another.\n",
    "\n",
    "4. Example of Laplace Smoothing in Action\n",
    "Consider a simple example where we are classifying documents into two classes: \"spam\" and \"not spam.\" If the word \"free\" appears in 5 spam documents and does not appear in any \"not spam\" documents, the naive calculation of the probability of \"free\" given \"not spam\" would be:\n",
    "\n",
    "[ P(\\text{\"free\"} | \\text{\"not spam\"}) = \\frac{0}{N_{\\text{\"not spam\"}}} = 0 ]\n",
    "\n",
    "With Laplace smoothing, we would calculate:\n",
    "\n",
    "[ P(\\text{\"free\"} | \\text{\"not spam\"}) = \\frac{0 + 1}{N_{\\text{\"not spam\"}} + V} ]\n",
    "\n",
    "This adjustment ensures that the model can still consider the \"not spam\" class when making predictions, even if the word \"free\" was not present in the training data for that class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988ac478-91eb-4a82-9524-cc251af6fb45",
   "metadata": {},
   "source": [
    "                                                             PRACTICLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ea7db8-5421-421f-b023-effcc6f27ba8",
   "metadata": {},
   "source": [
    "Q1>>Write a Python program to train an SVM Classifier on the Iris dataset and evaluate accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "222596c5-0195-47c8-8591-226237f6aae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\gajen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\gajen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from seaborn) (2.1.3)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\gajen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\gajen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from seaborn) (3.9.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\gajen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\gajen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\gajen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.55.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\gajen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\gajen\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\gajen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\gajen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\gajen\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\gajen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\gajen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\gajen\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\gajen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\gajen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\gajen\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\gajen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\gajen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\gajen\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\gajen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.1.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\gajen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.9.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\gajen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\gajen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\gajen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (4.55.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\gajen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\gajen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\gajen\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\gajen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\gajen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\gajen\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\gajen\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\gajen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\gajen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\gajen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\gajen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\gajen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (3.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn \n",
    "!pip install pandas \n",
    "!pip install numpy \n",
    "!pip install matplotlib\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62335c69-ae24-4d1c-b198-40986026bdaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM classifier on the Iris dataset: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data  # Features\n",
    "y = iris.target  # Target labels\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an SVM classifier\n",
    "svm_classifier = SVC(kernel='linear')  # You can also try 'rbf', 'poly', etc.\n",
    "\n",
    "# Train the classifier\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print the accuracy\n",
    "print(f'Accuracy of SVM classifier on the Iris dataset: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d41760-b624-4598-a7ee-6979355cb03a",
   "metadata": {},
   "source": [
    "Q2>>Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then\n",
    "compare their accuracies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39317865-0b6f-4ef6-9db9-f0f248c2eb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM classifier with Linear kernel: 100.00%\n",
      "Accuracy of SVM classifier with RBF kernel: 80.56%\n",
      "The Linear kernel performed better.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Wine dataset\n",
    "wine = datasets.load_wine()\n",
    "X = wine.data  # Features\n",
    "y = wine.target  # Target labels\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create SVM classifiers with different kernels\n",
    "svm_linear = SVC(kernel='linear')\n",
    "svm_rbf = SVC(kernel='rbf')\n",
    "\n",
    "# Train the classifiers\n",
    "svm_linear.fit(X_train, y_train)\n",
    "svm_rbf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_linear = svm_linear.predict(X_test)\n",
    "y_pred_rbf = svm_rbf.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracies\n",
    "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
    "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
    "\n",
    "# Print the accuracies\n",
    "print(f'Accuracy of SVM classifier with Linear kernel: {accuracy_linear * 100:.2f}%')\n",
    "print(f'Accuracy of SVM classifier with RBF kernel: {accuracy_rbf * 100:.2f}%')\n",
    "\n",
    "# Compare the accuracies\n",
    "if accuracy_linear > accuracy_rbf:\n",
    "    print(\"The Linear kernel performed better.\")\n",
    "elif accuracy_rbf > accuracy_linear:\n",
    "    print(\"The RBF kernel performed better.\")\n",
    "else:\n",
    "    print(\"Both kernels performed equally well.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6b0eae-46fb-4f3a-9da2-fbee41b23041",
   "metadata": {},
   "source": [
    "Q3>>Write a Python program to train an SVM Regressor (SVR) on a housing dataset and evaluate it using Mean\n",
    "Squared Error (MSE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d499ea-ed66-4bf1-9a4e-3d3b5d4eff5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the California housing dataset\n",
    "# Note: The California housing dataset is available in sklearn's datasets\n",
    "housing = datasets.fetch_california_housing()\n",
    "X = housing.data  # Features\n",
    "y = housing.target  # Target variable (house prices)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an SVR regressor\n",
    "svr_regressor = SVR(kernel='linear')  # You can also try 'rbf', 'poly', etc.\n",
    "\n",
    "# Train the regressor\n",
    "svr_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svr_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model using Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Print the Mean Squared Error\n",
    "print(f'Mean Squared Error of SVR on the California housing dataset: {mse:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0f5091-dc63-4ba7-9448-7a1dc528a5ce",
   "metadata": {},
   "source": [
    "Q4>>Write a Python program to train an SVM Classifier with a Polynomial Kernel and visualize the decision\n",
    "boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b6a178-8d18-49ee-a41d-cea224c25ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create a synthetic dataset using make_moons\n",
    "X, y = datasets.make_moons(n_samples=100, noise=0.1, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an SVM classifier with a polynomial kernel\n",
    "svm_classifier = SVC(kernel='poly', degree=3, C=1.0)  # You can adjust the degree and C parameter\n",
    "\n",
    "# Train the classifier\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Create a mesh grid for plotting decision boundaries\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
    "                     np.arange(y_min, y_max, 0.01))\n",
    "\n",
    "# Predict the class for each point in the mesh grid\n",
    "Z = svm_classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot the decision boundary\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.coolwarm)\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolors='k', marker='o', label='Training data')\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, edgecolors='k', marker='s', label='Test data')\n",
    "plt.title('SVM Classifier with Polynomial Kernel')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2bfe91-f83d-4659-a6bf-f66a30a728cb",
   "metadata": {},
   "source": [
    "Q5>>Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and\n",
    "evaluate accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64b9e1c-ae6e-46a4-af32-13aa4cc707b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "X = breast_cancer.data  # Features\n",
    "y = breast_cancer.target  # Target labels (0: malignant, 1: benign)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Gaussian Naïve Bayes classifier\n",
    "gnb_classifier = GaussianNB()\n",
    "\n",
    "# Train the classifier\n",
    "gnb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = gnb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print the accuracy\n",
    "print(f'Accuracy of Gaussian Naïve Bayes classifier on the Breast Cancer dataset: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8463c10f-7453-475d-a5d5-a212c0046400",
   "metadata": {},
   "source": [
    "Q6>>Write a Python program to train a Multinomial Naïve Bayes classifier for text classification using the 20\n",
    "Newsgroups dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd558450-0495-4fe1-9f9d-4a66222efdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load the 20 Newsgroups dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Extract features and labels\n",
    "X = newsgroups.data  # Text data\n",
    "y = newsgroups.target  # Target labels (newsgroup categories)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert text data to feature vectors using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_counts = vectorizer.fit_transform(X_train)\n",
    "X_test_counts = vectorizer.transform(X_test)\n",
    "\n",
    "# Create a Multinomial Naïve Bayes classifier\n",
    "mnb_classifier = MultinomialNB()\n",
    "\n",
    "# Train the classifier\n",
    "mnb_classifier.fit(X_train_counts, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = mnb_classifier.predict(X_test_counts)\n",
    "\n",
    "# Evaluate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print the accuracy\n",
    "print(f'Accuracy of Multinomial Naïve Bayes classifier on the 20 Newsgroups dataset: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Print classification report and confusion matrix\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=newsgroups.target_names))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a23899c-f060-446f-925a-0f08f0614eba",
   "metadata": {},
   "source": [
    "Q7>>Write a Python program to train an SVM Classifier with different C values and compare the decision\n",
    "boundaries visually="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bd828b-b99c-4337-9765-bfb85f5613bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create a synthetic dataset using make_moons\n",
    "X, y = datasets.make_moons(n_samples=100, noise=0.1, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define different C values to test\n",
    "C_values = [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "# Create a mesh grid for plotting decision boundaries\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
    "                     np.arange(y_min, y_max, 0.01))\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Train SVM classifiers with different C values and plot decision boundaries\n",
    "for i, C in enumerate(C_values):\n",
    "    # Create and train the SVM classifier\n",
    "    svm_classifier = SVC(kernel='linear', C=C)\n",
    "    svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the class for each point in the mesh grid\n",
    "    Z = svm_classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Plot the decision boundary\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.coolwarm)\n",
    "    plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolors='k', marker='o', label='Training data')\n",
    "    plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, edgecolors='k', marker='s', label='Test data')\n",
    "    plt.title(f'SVM with C={C}')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaa1cd2-48be-4439-acf8-d0c12572d56f",
   "metadata": {},
   "source": [
    "Q8>>Write a Python program to train a Bernoulli Naïve Bayes classifier for binary classification on a dataset with\n",
    "binary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1131b33b-76ca-4216-bef7-63bbc5747c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Create a synthetic binary classification dataset with binary features\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=0,\n",
    "                           n_clusters_per_class=1, random_state=42, n_classes=2)\n",
    "\n",
    "# Convert features to binary (0 or 1)\n",
    "X = (X > 0).astype(int)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Bernoulli Naïve Bayes classifier\n",
    "bnb_classifier = BernoulliNB()\n",
    "\n",
    "# Train the classifier\n",
    "bnb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = bnb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print the accuracy\n",
    "print(f'Accuracy of Bernoulli Naïve Bayes classifier: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Print classification report and confusion matrix\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef0433b-e902-4821-8e7a-dda3156e1464",
   "metadata": {},
   "source": [
    "Q9>>Write a Python program to apply feature scaling before training an SVM model and compare results with\n",
    "unscaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbb5b59-9485-4430-afc5-0b74bc5c48c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data  # Features\n",
    "y = iris.target  # Target labels\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train SVM on unscaled data\n",
    "svm_classifier_unscaled = SVC(kernel='linear')\n",
    "svm_classifier_unscaled.fit(X_train, y_train)\n",
    "y_pred_unscaled = svm_classifier_unscaled.predict(X_test)\n",
    "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
    "\n",
    "# Feature scaling using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train SVM on scaled data\n",
    "svm_classifier_scaled = SVC(kernel='linear')\n",
    "svm_classifier_scaled.fit(X_train_scaled, y_train)\n",
    "y_pred_scaled = svm_classifier_scaled.predict(X_test_scaled)\n",
    "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
    "\n",
    "# Print the results\n",
    "print(f'Accuracy of SVM classifier on unscaled data: {accuracy_unscaled * 100:.2f}%')\n",
    "print(f'Accuracy of SVM classifier on scaled data: {accuracy_scaled * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116d23d2-075d-460a-a590-0ba263e1a944",
   "metadata": {},
   "source": [
    "Q10>>Write a Python program to train a Gaussian Naïve Bayes model and compare the predictions before and\n",
    "after Laplace Smoothing="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2d7134-690a-4b2a-bcb8-b160c396d5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data  # Features\n",
    "y = iris.target  # Target labels\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Gaussian Naïve Bayes model without Laplace smoothing\n",
    "gnb_classifier = GaussianNB()\n",
    "gnb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_no_smoothing = gnb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy without smoothing\n",
    "accuracy_no_smoothing = accuracy_score(y_test, y_pred_no_smoothing)\n",
    "\n",
    "# Print results without smoothing\n",
    "print(\"Results without Laplace Smoothing:\")\n",
    "print(f'Accuracy: {accuracy_no_smoothing * 100:.2f}%')\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_no_smoothing))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_no_smoothing))\n",
    "\n",
    "# Simulate Laplace smoothing by adding a small constant to the features\n",
    "# Note: In Gaussian Naïve Bayes, we typically don't apply Laplace smoothing directly,\n",
    "# but we can simulate it by adding a small value to the features.\n",
    "X_train_smoothed = X_train + 0.1  # Adding a small constant\n",
    "X_test_smoothed = X_test + 0.1\n",
    "\n",
    "# Train Gaussian Naïve Bayes model with simulated Laplace smoothing\n",
    "gnb_classifier_smoothed = GaussianNB()\n",
    "gnb_classifier_smoothed.fit(X_train_smoothed, y_train)\n",
    "\n",
    "# Make predictions on the test set with smoothing\n",
    "y_pred_with_smoothing = gnb_classifier_smoothed.predict(X_test_smoothed)\n",
    "\n",
    "# Evaluate the accuracy with smoothing\n",
    "accuracy_with_smoothing = accuracy_score(y_test, y_pred_with_smoothing)\n",
    "\n",
    "# Print results with smoothing\n",
    "print(\"\\nResults with Simulated Laplace Smoothing:\")\n",
    "print(f'Accuracy: {accuracy_with_smoothing * 100:.2f}%')\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_with_smoothing))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_with_smoothing))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02b2e6d-a901-4d23-b60c-067f62e1b82c",
   "metadata": {},
   "source": [
    "Q11>>Write a Python program to train an SVM Classifier and use GridSearchCV to tune the hyperparameters (C,\n",
    "gamma, kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcb4398-0171-48dd-a6f3-d6c65907b1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data  # Features\n",
    "y = iris.target  # Target labels\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [0.01, 0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf', 'poly']\n",
    "}\n",
    "\n",
    "# Create an SVM classifier\n",
    "svm_classifier = SVC()\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=svm_classifier, param_grid=param_grid,\n",
    "                           scoring='accuracy', cv=5, verbose=1, n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and the best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f'Best Parameters: {best_params}')\n",
    "print(f'Best Cross-Validation Score: {best_score:.2f}')\n",
    "\n",
    "# Make predictions with the best estimator\n",
    "best_svm_classifier = grid_search.best_estimator_\n",
    "y_pred = best_svm_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc7f16e-b167-409a-aa58-69a33463df53",
   "metadata": {},
   "source": [
    "Q12>>Write a Python program to train an SVM Classifier on an imbalanced dataset and apply class weighting and\n",
    "check it improve accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e4cd1f-0747-463c-af5f-d9e277932290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Create a synthetic imbalanced dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=10,\n",
    "                           n_clusters_per_class=1, weights=[0.9, 0.1], flip_y=0, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train SVM classifier without class weighting\n",
    "svm_classifier = SVC(kernel='linear')\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_no_weight = svm_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy without class weighting\n",
    "accuracy_no_weight = accuracy_score(y_test, y_pred_no_weight)\n",
    "\n",
    "# Print results without class weighting\n",
    "print(\"Results without Class Weighting:\")\n",
    "print(f'Accuracy: {accuracy_no_weight * 100:.2f}%')\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_no_weight))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_no_weight))\n",
    "\n",
    "# Train SVM classifier with class weighting\n",
    "svm_classifier_weighted = SVC(kernel='linear', class_weight='balanced')\n",
    "svm_classifier_weighted.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set with class weighting\n",
    "y_pred_weight = svm_classifier_weighted.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy with class weighting\n",
    "accuracy_weight = accuracy_score(y_test, y_pred_weight)\n",
    "\n",
    "# Print results with class weighting\n",
    "print(\"\\nResults with Class Weighting:\")\n",
    "print(f'Accuracy: {accuracy_weight * 100:.2f}%')\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_weight))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6351c33c-7252-4b0c-84ef-316a4589040a",
   "metadata": {},
   "source": [
    "Q13>>Write a Python program to implement a Naïve Bayes classifier for spam detection using email data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04c3b4a-29bb-4163-82b7-3ca517c73a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load the 20 Newsgroups dataset\n",
    "# We will filter for spam-related categories\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.space']\n",
    "newsgroups = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({'text': newsgroups.data, 'target': newsgroups.target})\n",
    "\n",
    "# Convert target labels to binary (1 for spam, 0 for not spam)\n",
    "# For this example, let's consider 'alt.atheism' and 'soc.religion.christian' as spam (1)\n",
    "# and 'comp.graphics' and 'sci.space' as not spam (0)\n",
    "df['target'] = df['target'].apply(lambda x: 1 if x in [0, 1] else 0)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['target'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert text data to feature vectors using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_counts = vectorizer.fit_transform(X_train)\n",
    "X_test_counts = vectorizer.transform(X_test)\n",
    "\n",
    "# Create a Multinomial Naïve Bayes classifier\n",
    "mnb_classifier = MultinomialNB()\n",
    "\n",
    "# Train the classifier\n",
    "mnb_classifier.fit(X_train_counts, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = mnb_classifier.predict(X_test_counts)\n",
    "\n",
    "# Evaluate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print the accuracy\n",
    "print(f'Accuracy of Naïve Bayes classifier for spam detection: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Print classification report and confusion matrix\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1d3b34-1384-48f5-9e4e-e95bb16a7998",
   "metadata": {},
   "source": [
    "Q14>>Write a Python program to train an SVM Classifier and a Naïve Bayes Classifier on the same dataset and\n",
    "compare their accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bb16ed-9bcb-4be5-aaa4-120f45836a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data  # Features\n",
    "y = iris.target  # Target labels\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train SVM Classifier\n",
    "svm_classifier = SVC(kernel='linear')  # You can also try 'rbf', 'poly', etc.\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set using SVM\n",
    "y_pred_svm = svm_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of SVM\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "\n",
    "# Print results for SVM\n",
    "print(\"SVM Classifier Results:\")\n",
    "print(f'Accuracy: {accuracy_svm * 100:.2f}%')\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_svm))\n",
    "\n",
    "# Train Naïve Bayes Classifier\n",
    "nb_classifier = GaussianNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set using Naïve Bayes\n",
    "y_pred_nb = nb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of Naïve Bayes\n",
    "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
    "\n",
    "# Print results for Naïve Bayes\n",
    "print(\"\\nNaïve Bayes Classifier Results:\")\n",
    "print(f'Accuracy: {accuracy_nb * 100:.2f}%')\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_nb))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_nb))\n",
    "\n",
    "# Compare accuracies\n",
    "print(\"\\nComparison of Accuracies:\")\n",
    "print(f'SVM Accuracy: {accuracy_svm * 100:.2f}%')\n",
    "print(f'Naïve Bayes Accuracy: {accuracy_nb * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff34ba73-f5cc-417f-a4f9-400cc5101b14",
   "metadata": {},
   "source": [
    "Q15>>Write a Python program to perform feature selection before training a Naïve Bayes classifier and compare\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdc4171-4bdc-412b-a6a3-6a85fb76a4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data  # Features\n",
    "y = iris.target  # Target labels\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Naïve Bayes Classifier on all features\n",
    "nb_classifier_all = GaussianNB()\n",
    "nb_classifier_all.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set using all features\n",
    "y_pred_all = nb_classifier_all.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of Naïve Bayes with all features\n",
    "accuracy_all = accuracy_score(y_test, y_pred_all)\n",
    "\n",
    "# Print results for Naïve Bayes with all features\n",
    "print(\"Naïve Bayes Classifier Results (All Features):\")\n",
    "print(f'Accuracy: {accuracy_all * 100:.2f}%')\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_all))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_all))\n",
    "\n",
    "# Perform feature selection\n",
    "selector = SelectKBest(score_func=f_classif, k=2)  # Select the top 2 features\n",
    "X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "# Train Naïve Bayes Classifier on selected features\n",
    "nb_classifier_selected = GaussianNB()\n",
    "nb_classifier_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Make predictions on the test set using selected features\n",
    "y_pred_selected = nb_classifier_selected.predict(X_test_selected)\n",
    "\n",
    "# Evaluate the accuracy of Naïve Bayes with selected features\n",
    "accuracy_selected = accuracy_score(y_test, y_pred_selected)\n",
    "\n",
    "# Print results for Naïve Bayes with selected features\n",
    "print(\"\\nNaïve Bayes Classifier Results (Selected Features):\")\n",
    "print(f'Accuracy: {accuracy_selected * 100:.2f}%')\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_selected))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_selected))\n",
    "\n",
    "# Compare accuracies\n",
    "print(\"\\nComparison of Accuracies:\")\n",
    "print(f'Accuracy with All Features: {accuracy_all * 100:.2f}%')\n",
    "print(f'Accuracy with Selected Features: {accuracy_selected * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e001966-29e4-44df-ad4e-f8f9e4121345",
   "metadata": {},
   "source": [
    "Q16>>Write a Python program to train an SVM Classifier using One-vs-Rest (OvR) and One-vs-One (OvO)\n",
    "strategies on the Wine dataset and compare their accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42c89c8-dab7-401c-a454-d91fec0de938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "\n",
    "# Load the Wine dataset\n",
    "wine = datasets.load_wine()\n",
    "X = wine.data  # Features\n",
    "y = wine.target  # Target labels\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train SVM Classifier using One-vs-Rest (OvR) strategy\n",
    "ovr_classifier = OneVsRestClassifier(SVC(kernel='linear', random_state=42))\n",
    "ovr_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set using OvR\n",
    "y_pred_ovr = ovr_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of OvR\n",
    "accuracy_ovr = accuracy_score(y_test, y_pred_ovr)\n",
    "\n",
    "# Print results for OvR\n",
    "print(\"One-vs-Rest (OvR) Classifier Results:\")\n",
    "print(f'Accuracy: {accuracy_ovr * 100:.2f}%')\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_ovr))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_ovr))\n",
    "\n",
    "# Train SVM Classifier using One-vs-One (OvO) strategy\n",
    "ovo_classifier = OneVsOneClassifier(SVC(kernel='linear', random_state=42))\n",
    "ovo_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set using OvO\n",
    "y_pred_ovo = ovo_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of OvO\n",
    "accuracy_ovo = accuracy_score(y_test, y_pred_ovo)\n",
    "\n",
    "# Print results for OvO\n",
    "print(\"\\nOne-vs-One (OvO) Classifier Results:\")\n",
    "print(f'Accuracy: {accuracy_ovo * 100:.2f}%')\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_ovo))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_ovo))\n",
    "\n",
    "# Compare accuracies\n",
    "print(\"\\nComparison of Accuracies:\")\n",
    "print(f'OvR Accuracy: {accuracy_ovr * 100:.2f}%')\n",
    "print(f'OvO Accuracy: {accuracy_ovo * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1521c9-161d-4c10-8a88-be9e7c288cb8",
   "metadata": {},
   "source": [
    "Q16>>Write a Python program to train an SVM Classifier using Linear, Polynomial, and RBF kernels on the Breast\n",
    "Cancer dataset and compare their accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a447462-12e0-4d8a-9ebe-78a2310c1224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "X = breast_cancer.data  # Features\n",
    "y = breast_cancer.target  # Target labels (0: malignant, 1: benign)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a function to train and evaluate SVM with different kernels\n",
    "def train_and_evaluate_svm(kernel):\n",
    "    # Create an SVM classifier with the specified kernel\n",
    "    svm_classifier = SVC(kernel=kernel, random_state=42)\n",
    "    \n",
    "    # Train the classifier\n",
    "    svm_classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = svm_classifier.predict(X_test)\n",
    "    \n",
    "    # Evaluate the accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nSVM Classifier Results with {kernel.capitalize()} Kernel:\")\n",
    "    print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Train and evaluate SVM with Linear kernel\n",
    "train_and_evaluate_svm(kernel='linear')\n",
    "\n",
    "# Train and evaluate SVM with Polynomial kernel\n",
    "train_and_evaluate_svm(kernel='poly')\n",
    "\n",
    "# Train and evaluate SVM with RBF kernel\n",
    "train_and_evaluate_svm(kernel='rbf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cfdf65-7708-4639-b46d-60a2e1e71643",
   "metadata": {},
   "source": [
    "Q18>>Write a Python program to train an SVM Classifier using Stratified K-Fold Cross-Validation and compute the\n",
    "average accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5605621-2d94-4ab8-9373-74b2fe760937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "X = breast_cancer.data  # Features\n",
    "y = breast_cancer.target  # Target labels (0: malignant, 1: benign)\n",
    "\n",
    "# Create an SVM classifier\n",
    "svm_classifier = SVC(kernel='linear', random_state=42)\n",
    "\n",
    "# Set up Stratified K-Fold Cross-Validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation and compute accuracy for each fold\n",
    "accuracies = cross_val_score(svm_classifier, X, y, cv=skf, scoring='accuracy')\n",
    "\n",
    "# Compute the average accuracy\n",
    "average_accuracy = np.mean(accuracies)\n",
    "\n",
    "# Print the results\n",
    "print(f'Accuracies for each fold: {accuracies}')\n",
    "print(f'Average accuracy across all folds: {average_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d537b24-7084-4ace-99fc-8a9e746135f1",
   "metadata": {},
   "source": [
    "Q19>>Write a Python program to train a Naïve Bayes classifier using different prior probabilities and compare\n",
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b523d0-15a5-4d5a-8ea3-22caae281d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data  # Features\n",
    "y = iris.target  # Target labels\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to train and evaluate Naïve Bayes with given priors\n",
    "def train_and_evaluate_nb(priors):\n",
    "    # Create a Gaussian Naïve Bayes classifier with specified priors\n",
    "    nb_classifier = GaussianNB(priors=priors)\n",
    "    \n",
    "    # Train the classifier\n",
    "    nb_classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = nb_classifier.predict(X_test)\n",
    "    \n",
    "    # Evaluate the accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nNaïve Bayes Classifier Results with Priors {priors}:\")\n",
    "    print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Define different prior probabilities\n",
    "priors_list = [\n",
    "    [1/3, 1/3, 1/3],  # Uniform priors\n",
    "    [0.5, 0.3, 0.2],  # Custom priors\n",
    "    [0.2, 0.5, 0.3]   # Another set of custom priors\n",
    "]\n",
    "\n",
    "# Train and evaluate Naïve Bayes with different priors\n",
    "for priors in priors_list:\n",
    "    train_and_evaluate_nb(priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f12c44b-eb8a-4fbb-90f9-383a9d22a455",
   "metadata": {},
   "source": [
    "Q20>>Write a Python program to perform Recursive Feature Elimination (RFE) before training an SVM Classifier and\n",
    "compare accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ed5b98-a13f-4260-ac94-6e3ddd77bb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "X = breast_cancer.data  # Features\n",
    "y = breast_cancer.target  # Target labels (0: malignant, 1: benign)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train SVM Classifier on all features\n",
    "svm_classifier_all = SVC(kernel='linear', random_state=42)\n",
    "svm_classifier_all.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set using all features\n",
    "y_pred_all = svm_classifier_all.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of SVM with all features\n",
    "accuracy_all = accuracy_score(y_test, y_pred_all)\n",
    "\n",
    "# Print results for SVM with all features\n",
    "print(\"SVM Classifier Results (All Features):\")\n",
    "print(f'Accuracy: {accuracy_all * 100:.2f}%')\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_all))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_all))\n",
    "\n",
    "# Perform Recursive Feature Elimination (RFE)\n",
    "svm_classifier_rfe = SVC(kernel='linear', random_state=42)\n",
    "rfe = RFE(estimator=svm_classifier_rfe, n_features_to_select=5)  # Select top 5 features\n",
    "X_train_rfe = rfe.fit_transform(X_train, y_train)\n",
    "X_test_rfe = rfe.transform(X_test)\n",
    "\n",
    "# Train SVM Classifier on selected features\n",
    "svm_classifier_rfe.fit(X_train_rfe, y_train)\n",
    "\n",
    "# Make predictions on the test set using selected features\n",
    "y_pred_rfe = svm_classifier_rfe.predict(X_test_rfe)\n",
    "\n",
    "# Evaluate the accuracy of SVM with selected features\n",
    "accuracy_rfe = accuracy_score(y_test, y_pred_rfe)\n",
    "\n",
    "# Print results for SVM with selected features\n",
    "print(\"\\nSVM Classifier Results (Selected Features with RFE):\")\n",
    "print(f'Accuracy: {accuracy_rfe * 100:.2f}%')\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_rfe))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_rfe))\n",
    "\n",
    "# Compare accuracies\n",
    "print(\"\\nComparison of Accuracies:\")\n",
    "print(f'Accuracy with All Features: {accuracy_all * 100:.2f}%')\n",
    "print(f'Accuracy with Selected Features (RFE): {accuracy_rfe * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2123ac4-100b-49fe-a090-aece2f0ec1ad",
   "metadata": {},
   "source": [
    "Q21>>>Write a Python program to train an SVM Classifier and evaluate its performance using Precision, Recall, and\n",
    "F1-Score instead of accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c545f0da-d155-4a9e-8aa4-f8e1881df39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "X = breast_cancer.data  # Features\n",
    "y = breast_cancer.target  # Target labels (0: malignant, 1: benign)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an SVM classifier\n",
    "svm_classifier = SVC(kernel='linear', random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate performance using Precision, Recall, and F1-Score\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(\"SVM Classifier Performance:\")\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1-Score: {f1:.2f}')\n",
    "\n",
    "# Print classification report and confusion matrix\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cf6368-38f6-41ea-a025-6c3c2108e70e",
   "metadata": {},
   "source": [
    "Q42>>Write a Python program to train a Naïve Bayes Classifier and evaluate its performance using Log Loss\n",
    "(Cross-Entropy Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3082773-e863-4681-a792-b189fe995f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import log_loss, classification_report, confusion_matrix\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "X = breast_cancer.data  # Features\n",
    "y = breast_cancer.target  # Target labels (0: malignant, 1: benign)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Gaussian Naïve Bayes classifier\n",
    "nb_classifier = GaussianNB()\n",
    "\n",
    "# Train the classifier\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_proba = nb_classifier.predict_proba(X_test)  # Get predicted probabilities\n",
    "y_pred = nb_classifier.predict(X_test)  # Get predicted classes\n",
    "\n",
    "# Evaluate performance using Log Loss\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "\n",
    "# Print the results\n",
    "print(\"Naïve Bayes Classifier Performance:\")\n",
    "print(f'Log Loss: {log_loss_value:.4f}')\n",
    "\n",
    "# Print classification report and confusion matrix\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d78591a-9ec9-4152-b98e-f4824abb220d",
   "metadata": {},
   "source": [
    "Q24>>Write a Python program to train an SVM Classifier and visualize the Confusion Matrix using seaborn="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c176e8c-2536-4041-a49b-dbc43b1cb797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "X = breast_cancer.data  # Features\n",
    "y = breast_cancer.target  # Target labels (0: malignant, 1: benign)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an SVM classifier\n",
    "svm_classifier = SVC(kernel='linear', random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Visualize the confusion matrix using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Malignant', 'Benign'], yticklabels=['Malignant', 'Benign'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ccbaac-b716-4d65-8610-31b3fc32dbf9",
   "metadata": {},
   "source": [
    "Q25>>Write a Python program to train an SVM Regressor (SVR) and evaluate its performance using Mean Absolute\n",
    "Error (MAE) instead of MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0797d0b6-e819-438b-9084-a7b9e156dd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Create a synthetic regression dataset\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an SVR regressor\n",
    "svr_regressor = SVR(kernel='linear')\n",
    "\n",
    "# Train the regressor\n",
    "svr_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svr_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate performance using Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(\"SVR Regressor Performance:\")\n",
    "print(f'Mean Absolute Error (MAE): {mae:.4f}')\n",
    "\n",
    "# Optionally, you can also print the Mean Squared Error (MSE) for comparison\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error (MSE): {mse:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acfc07b-a6d3-47aa-b521-c7e3e40fb4de",
   "metadata": {},
   "source": [
    "Q25>>Write a Python program to train a Naïve Bayes classifier and evaluate its performance using the ROC-AUC\n",
    "score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b979a076-09d8-4408-a58b-66ccb2e251fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "X = breast_cancer.data  # Features\n",
    "y = breast_cancer.target  # Target labels (0: malignant, 1: benign)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Gaussian Naïve Bayes classifier\n",
    "nb_classifier = GaussianNB()\n",
    "\n",
    "# Train the classifier\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_proba = nb_classifier.predict_proba(X_test)[:, 1]  # Get predicted probabilities for the positive class\n",
    "\n",
    "# Evaluate performance using ROC-AUC score\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Print the ROC-AUC score\n",
    "print(f'ROC-AUC Score: {roc_auc:.4f}')\n",
    "\n",
    "# Optionally, plot the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', label='ROC Curve (area = {:.2f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--')  # Diagonal line\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49658a64-f9ec-4622-8e7a-0a87a57b3e5e",
   "metadata": {},
   "source": [
    "Q26>>Write a Python program to train an SVM Classifier and visualize the Precision-Recall Curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcea6b3f-006b-490c-8494-8010da4cf996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "X = breast_cancer.data  # Features\n",
    "y = breast_cancer.target  # Target labels (0: malignant, 1: benign)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an SVM classifier\n",
    "svm_classifier = SVC(kernel='linear', probability=True, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_scores = svm_classifier.predict_proba(X_test)[:, 1]  # Get predicted probabilities for the positive class\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
    "\n",
    "# Calculate average precision score\n",
    "average_precision = average_precision_score(y_test, y_scores)\n",
    "\n",
    "# Plot the Precision-Recall curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, color='blue', label='Precision-Recall curve (AP = {:.2f})'.format(average_precision))\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.grid()\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258fa304-30c0-41e3-84b0-47ee945af744",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
